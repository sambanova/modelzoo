= Walkthrough--Inference and Fine-tuning with Llama-2-7b for Chat [[Walkthrough]]

The Model Zoo NLP examples illustrate how to run training and text generation (inference) on RDU. You can explore how the code for running SambaNova RDU differs from the corresponding code for running on CPU. In this initial release, these examples are not intended for use in production because they are not tuned for performance.

NOTE: The CPU app is intended to be used as an example, it is tested only with Llama-2-7b. It is not guaranteed to work outside of the DevBox environment in typical user workflows.

In this walkthrough, you learn how to run inference and fine tuning. Depending on the tasks you want to perform next, you can go through both of these tasks or just one of them. 

See link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[Model Zoo Best Practices] for additional information, including link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html#_making_changes_to_model_zoo_models[Making Changes to Model Zoo Models].

== Prerequisites

Before you start the walkthrough, you have to set up the development container, install app dependencies, and download a model checkpoint from Hugging Face. For the training task, you also have to perform data preparation. 

=== Step 1: Set up the Development Container

You use a container that we call a Devbox to compile and run the model. See the xref:../../docs/container-setup.adoc[Container Setup Guide] for step-by-step instructions. 

=== Step 2: Install App Dependencies

From within your development container, install Model Zoo in editable mode and its Python dependencies:


```bash
cd /opt/modelzoo && pip install -r requirements/requirements.txt && pip install -e .;
```

With editable mode installation, source code edits are immediately reflected during interactive development.  

=== Step 3: Download a Model Checkpoint from Hugging Face


NOTE: Model Zoo models are compatible with any Hugging Face bfloat16 or float32 checkpoints. For a list of tested checkpoints, see https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html.

You will use the checkpoint for inference, and as a starting point for fine-tuning.

To download the Llama-2-7b checkpoint for this walkthrough:

. Make sure https://git-lfs.com[Git LFS] is installed: `git lfs install`.
. Create a https://huggingface.co/join[Hugging Face account].
. Go to https://huggingface.co/meta-llama/Llama-2-7b-hf[meta-llama/Llama-2-7b-hf] and accept the terms of use for Llama-2-7b.
. In your https://huggingface.co/settings/tokens[Hugging Face account settings], generate a link:https://huggingface.co/docs/hub/en/security-tokens[user access token].
. Clone the model:
.. `git clone \https://huggingface.co/meta-llama/Llama-2-7b-hf`
.. Enter your access token when prompted

[NOTE]
====
Downloading the checkpoint can take several minutes without any output.
You can instead download the https://huggingface.co/meta-llama/Llama-2-7b-hf[model files] manually and omit the `.bin` files to save space.
====


== Task 1: Run Inference - Generate Text Using a Checkpoint

In this task, you compile the model so it can run efficiently on RDU. Then you perform an inference run and see that the model returns an appropriate response to a prompt. The prompt is initially predefined in the `text_generation/config/base_config_rdu.yaml` file. 

NOTE: If you're primarily interested in fine-tuning the model, follow the steps in <<Task 2: Run Training - Fine-tune the Model using a Chat Dataset>>. 

=== Step 0: Set up Your Environment. 

Follow the instructions in *<<Prerequisites>>* above. 

// Next sections until Task 2 came from /examples/text_generation/README

=== Step 1: Compile the Example Model and Generate a PEF

NOTE: A PEF is an executable file outputted after compiling the model. For more information on PEF and our compiler see link:https://docs.sambanova.ai/developer/latest/compiler-overview.html[SambaFlow compiler overview].

. Move into the app directory:
+
[source,bash]
----
cd /opt/modelzoo/examples/nlp/text_generation/
----

. Compile the model:
+
[source,bash]
----
python rdu_generate_text.py \
  command=compile \
  checkpoint.model_name_or_path=PATH_TO_DOWNLOADED_MODEL \
  samba_compile.output_folder=PATH_TO_OUTPUT \
  +samba_compile.target_sambaflow_version=MAJOR.MINOR.PATCH
----
+
* `PATH_TO_DOWNLOADED_MODEL` should point to the container paths for the checkpoint. You can also use a checkpoint generated by the training example app (see Task 2, <<Step 4: Run Inference with your Checkpoint>>).
* `PATH_TO_OUTPUT` should point to a writable output folder, the root of the modelzoo repo by default. For this folder to persist outside the container, ensure that the folder is bind mounted from the host by following the steps in the xref:../../docs/container-setup.adoc[Container Setup Guide].
* Use the following command outside the container to check the version of the `sambanova-runtime` package on the host machine using either rpm or dpkg:
+
[source,bash]
----
(rpm -q sambanova-runtime 2>/dev/null || dpkg -s sambanova-runtime 2>/dev/null) | egrep -m 1 -o "[0-9]+\.[0-9]+\.[0-9]+"
----
* If your host machine is on an older version of sambanova-runtime than the PEF, add `+samba_compile.target_sambaflow_version=MAJOR.MINOR.PATCH` to your command. 

+
NOTE: When using the Hydra command-line syntax, you must add a `+` for arguments not defined in `config/base_config_rdu.yaml`. For parameters already in the YAML file, you can either modify the YAML directly or override them on the command line without the `+`. See link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[How to pass in arguments] for additional guidance.

The compiler writes a PEF file (`.pef`) to the output directory specified by PATH_TO_OUTPUT.

=== Step 2: Run Generation with the PEF

. Run the model with the newly obtained PEF:
+
[source,bash]
----
python rdu_generate_text.py \
  command=run \
  checkpoint.model_name_or_path=PATH_TO_DOWNLOADED_MODEL \
  samba_run.pef=PATH_TO_PEF
----
+
Adjust `PATH_TO_DOWNLOADED_MODEL` and `PATH_TO_PEF` to match your environment. If the length of outputs is limited by `max_new_tokens` in `config/base_config_rdu.yaml`, remove that limit to enable the maximum number of outputs.

+
By default, the example runs with the prompt defined in `config/base_config_rdu.yaml`:
+
----
Once upon a time
----

. The output, that is, the completion for that prompt, could look like the following:
+
----
, there was a little girl who loved to read. She loved to read so much that she would
----

. You can now run the model with other prompts as input. To try a prompt that is different from the default, use this template:
+
[source,bash]
----
python rdu_generate_text.py \
command=run \ 
checkpoint.model_name_or_path=PATH_TO_DOWNLOADED_MODEL \ 
samba_run.pef=PATH_TO_PEF \
generation.prompts=['YOUR_PROMPT_HERE']
----
+
NOTE: If you are using a batch size greater than 1, ensure that the number of prompts matches the batch size. You may add this as an item to the `prompts` array in `config/base_config_rdu.yaml` or directly in the run command. For example, with a batch size of 2:
+
[source,bash]
----
python rdu_generate_text.py \
command=run \ 
checkpoint.model_name_or_path=PATH_TO_DOWNLOADED_MODEL \ 
samba_run.pef=PATH_TO_PEF \
'generation.prompts=["Once upon a time", "The tallest mountain"]'
----

At the end of a text generation run, the app saves a checkpoint and outputs some basic telemetry and performance metrics to a summary.txt file with information like the following: 

    latencies
        time to first token 1.2131s
        tokens,  excluding first token 0.3460s
        tokens,  overall 0.3731s
        Total Latency 1.5592s
    throughputs
        tokens/second excluding first token 2.8899
        tokens/second overall 2.6800

== Task 2: Run Training - Fine-tune the Model using a Chat Dataset

In this task, you fine-tune link:https://llama.meta.com/llama2/[Llama-2-7b]. You prepare a dataset, compile the model, and fine-tune the model with the dataset to see if you can improve the domain-specific accuracy of the model. The training parameters are set in the `training/config/base_config_rdu.yaml` file.

NOTE: If you're primarily interested in text generation (inference), follow the steps in <<Task 1: Run Inference - Generate Text Using a Checkpoint>>. 

=== Step 0: Set up Your Environment. 

Follow the instructions in *<<Prerequisites>>* above. 

=== Step 1: Data Preparation

In addition to a checkpoint (downloaded as part of *<<Prerequisites>>* above), you need a dataset to fine-tune your model on.

.About Generative Data Prep
[%collapsible]
====

[sidebar]
.The Role of Generative Data Prep
--
The generative data prep package referenced in this section is used at SambaNova internally for all LLM training processes.

It is responsible for:

* Reading large amounts of text data split into articles.
* Randomly shuffling text on article boundaries.
	** Text from the same article stays contiguous and in order.
	** But the order of articles is randomized.
* Splitting data into `train/dev/test`.
* Tokenizing and encoding text from the articles.
* Packing encodings efficiently to maximize text and minimize padding in each batch.
	This is done by creating a more efficient __3D attention mask__ input to the model using the `token type ids` to delimit sequences.
. Organizing batches to be read by multiple training workers in data parallel mode.
--
====

.Example Input
[%collapsible]
====

The following is a subset of an example input file for multi-turn chat data.

[source,json,title='input.jsonl']
----
[{"prompt": "What's your favorite season?", "completion": "I love fall"}, {"prompt": "Oh yeah, me too. What do you like about it?", "completion": "I love the cool weather and the changing leaves"}] <1> <2>
[{"prompt": "What is your favorite hobby?", "completion": "My favorite hobby is reading."},<3>
{"prompt": "That's interesting. What book are you currently reading?", "completion": "I am currently reading 'The Catcher in the Rye' by J.D. Salinger."}]
----
<1> Each line in the `.jsonl` file is a JSON value, in this case a list.
<2> Each list represents one article. The order of the prompt-completion pairs will not be shuffled.
<3> `prompt` and `completion` are provided separately to provide more importance (weight) to the completion during training.

See the https://github.com/SambaNova/generative_data_prep/blob/main/examples/dialogue/example_dialogue_data.jsonl[complete file] for this example and others in the examples section of Generative Data Prep's https://github.com/SambaNova/generative_data_prep#examples[README].

This example file, when processed, outputs a folder that contains:

[source,title='output/']
----
train_1_of_4.hdf5
train_2_of_4.hdf5
train_3_of_4.hdf5
train_4_of_4.hdf5
dev_1_of_1.hdf5
----

Each `.hdf5` file contains several batches, which themselves contain several text sequences. Each sequence contains an `input_id` and `token_type_id` tensor (used to generate the special attention mask).

Using this output, you can train with up to 4 workers in parallel (if in data parallel mode). If you have 32 files, you can train using 1, 2, 4, 8, 16, or 32 workers.
====

You can prepare any `.txt` or `.jsonl` dataset for training. This example uses the https://github.com/thunlp/UltraChat[UltraChat] dataset. UltraChat is an open source, multi-turn dialogue dataset.

==== Process the Dataset

. Install the https://github.com/sambanova/generative_data_prep[Generative Data Prep] package in a virtualenv:
+
[source,bash]
----
cd /opt
git clone https://github.com/sambanova/generative_data_prep.git
cd generative_data_prep
python -m venv env
source env/bin/activate
pip install .
----

. Download UltraChat from its https://huggingface.co/datasets/stingning/ultrachat[Hugging Face page]:
+
NOTE: Ensure that you have `git lfs` installed with `yum install git-lfs && git lfs install` (or `apt` for `ubuntu`) before cloning. If `git lfs` is not installed, the `git clone` command will not download the full files. If that happens, the ultrachat folder will be 544kb in size and the next steps will produce empty files.
+
[source,bash]
----
cd /opt/modelzoo/examples/nlp/training # or a directory where you can download the dataset
git clone https://huggingface.co/datasets/stingning/ultrachat
----
. Convert the dataset to the `.jsonl` format expected by Generative Data Prep. For this dataset, use the utility script included in this example.
	** Run: `python utils/convert_ultrachat.py -src ultrachat/ -dest ultrachat_processed.jsonl`
    ** You will likely see warnings such as `Skipped 1 line due to errors`. These warnings are caused by incorrectly formatted data in the dataset and can be safely ignored because those samples are dropped from processing.
	** See the (collapsed) example input at the beginning of this section for more information about this format.
. (Optionally), reduce the size of the dataset for a test run.
	** When using the full dataset, it takes several hours to complete training.
	** To run a quicker training job for testing, trim the dataset to the first 10,000 samples. Run this command (then continue following the instructions without modification): 
+
[source,bash]
----
mv ultrachat_processed.jsonl ultrachat_processed_full.jsonl
head -10000 ultrachat_processed_full.jsonl > ultrachat_processed.jsonl
----
+

. Run Generative Data Prep to convert your chat data from `.jsonl` to tokenized `.hdf5` files.
	** See the https://github.sambanovasystems.com/SambaNova/generative_data_prep#flags[source repo] for the full list of arguments.
+
[source,bash]
----
export TOKENIZER="./Llama-2-7b-hf"  # The location of your model
export MAX_SEQ_LEN=4096  # The sequence length of your model
python3 -m generative_data_prep pipeline \
	--input_file_path=ultrachat_processed.jsonl \
	--output_path=ultrachat_dialogue \
	--pretrained_tokenizer=${TOKENIZER} \
	--max_seq_length=${MAX_SEQ_LEN} \
	--input_packing_config='single::truncate_right' \
	--dev_ratio=0.1 \
	--shuffle=on_RAM
----
. Deactivate the virtualenv when processing is finished.
+
`deactivate`

=== Step 2: Compile the Example Model and Generate a PEF

NOTE: A PEF is an executable file outputted after compiling the model. For more information on PEF and our compiler see link:https://docs.sambanova.ai/developer/latest/compiler-overview.html[SambaFlow compiler overview].

. Set environment variables, for example:
+
```bash
export CHECKPOINT=./Llama-2-7b-hf
export MAX_SEQ_LENGTH=4096
export BATCH_SIZE=8
export ARCH=sn30
```
+
* `CHECKPOINT` can be any of the following:
    ** a path to a `config.json` for pretraining from scratch
    ** a path to a checkpoint folder for finetunting
    ** a model identifier on https://huggingface.co[huggingface.co]
* `MAX_SEQ_LENGTH` is the maximum sequence length of your chosen model.
* `BATCH_SIZE` is the batch size of data to use for training.
* `ARCH` is the target RDU architecture (e.g. `sn30` or `sn40`).

. Ensure you're in the app directory:
+
[source,bash]
----
cd /opt/modelzoo/examples/nlp/training
----
. Compile to generate a PEF:
+
[source,bash]
----
python rdu_train_llm.py \
    command=compile \
    checkpoint.config_name=${CHECKPOINT} \
    model.max_seq_length=${MAX_SEQ_LENGTH} \
    training.batch_size=${BATCH_SIZE} \
    samba_compile.arch=${ARCH} \
----

+
* Use the following command outside the container to check the version of the `sambanova-runtime` package on the host machine using either rpm or dpkg:
+
[source,bash]
----
(rpm -q sambanova-runtime 2>/dev/null || dpkg -s sambanova-runtime 2>/dev/null) | egrep -m 1 -o "[0-9]+\.[0-9]+\.[0-9]+"
----
* If your host machine is on an older version of sambanova-runtime than the PEF, add `+samba_compile.target_sambaflow_version=MAJOR.MINOR.PATCH` to your command. 

+
NOTE: When using the Hydra command-line syntax, you must add a `+` for arguments not defined in `config/base_config_rdu.yaml`. For parameters already in the YAML file, you can either modify the YAML directly or override them on the command line without the `+`. See link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[How to pass in arguments] for additional guidance.

=== Step 3: Run Training with the PEF

. Ensure that all <<Prerequisites>> have been met, the checkpoint has been downloaded, and the dataset has been processed.
. Run the training example app:

[source,bash]
----
export DATASET=./ultrachat_dialogue;  # or container path to dataset
export PEF=/path/to/compiled/pef;
python -u rdu_train_llm.py \
    command=run \
    checkpoint.model_name_or_path=${CHECKPOINT} \
    model.max_seq_length=${MAX_SEQ_LENGTH} \
    samba_run.pef=${PEF} \
    training.dataset=${DATASET}
----

TIP: By default, training runs for 1 epoch (sees all samples once). To run for more epochs, modify the config file or use training.num_epochs=3. To run only for a few steps for testing, use +training.end_early_at_step=100. 

==== Output

At the end of training, the example app saves a checkpoint and a `summary.txt` file at the location defined inside the `config/base_config_rdu.yaml` configuration file. By default, this location is set to `finetuned_model` in the current working directory. After training, this folder contains the following:

* A Hugging Face format checkpoint.
* A `summary.txt` file with information like the following: 

    Number of epochs: 1
    Per worker batch size: 2
    Per worker number of batches (steps): 2
    Number of DP workers: 2
    Total tokens seen: 4914
    Tokens per second: 120.8163
    Average time per step: 20.3309s 
    The following are the model params used to train this model using Model Zoo:{"fp32_ln":false,"fp32_logits":true,"fp32_skip_add":true,"mixedp_attn":true,"max_seq_length":4096,"use_plugin_heuristics":false,"use_segmented_softmax_attn":false}

* A `per_step_metrics.csv` file with information like the following: 

    Tokens in Step,Step Loss,Learning Rate,Time per Step
    tensor(2691),tensor(0.9211),1e-05,20.194304943084717 
    tensor(2223),tensor(0.2960),1e-05,20.467589616775513

==== Modifications to Training

* *Random weights*: To initialize weights for pretraining randomly (instead of from disk) use `checkpoint.config_name` instead of `checkpoint.model_name_or_path`.

* *Running Llama-2-70b*. With most supported models, you can run training with the command above and the default values in `config/base_config_rdu.yaml`. The only exception is Llama-2-70b, which requires that you use Tensor Parallel to ensure the model fits on the RDU. Use these settings in the `samba_compile` section of the `config/base_config_rdu.yaml` file:

    samba_compile:
    tensor_parallel: weight
    n_chips: 2
    num_tiles: 8
    early_tp: true


=== Step 4: Run Inference with your Checkpoint

You can use the saved Hugging Face format checkpoint from `finetuned_model/` to run inference with the finetuned model weights. Here are the steps:

. Copy the `tokenizer.json` file from the original Hugging Face checkpoint (set above as `$CHECKPOINT`), to the `finetuned_model` folder.

. Compile an inference PEF using the `config.json` created inside the `finetuned_model` folder.
+
[source,bash]
----
cd /opt/modelzoo/examples/nlp/text_generation/
python rdu_generate_text.py \
    command=compile \
    checkpoint.model_name_or_path=<PATH_TO_FINETUNED_MODEL> \
    model.max_seq_length=${MAX_SEQ_LENGTH} \
    generation.batch_size=${BATCH_SIZE} \
    samba_compile.arch=${ARCH} \
    +samba_compile.target_sambaflow_version=MAJOR.MINOR.PATCH
----
+
NOTE: Make sure to use the same flags for compiling the inference PEF that were used to compile the training PEF. For example, if the training PEF was compiled with a given `training.batch_size` or `model.max_seq_length`, the command above must use the same `generation.batch_size` and `model.max_seq_length`.

. Update the prompts in the `text_generation/config/base_config_rdu.yaml` file. For example, with a batch size of 8, the `generation.prompts` field of `text_generation/config/base_config_rdu.yaml` should include 8 entries for 8 distinct prompts.

. Run the inference PEF with the fine-tuned checkpoint
+
[source,bash]
----
export PEF=path/to/inference.pef;
python rdu_generate_text.py \
    command=run \
    checkpoint.model_name_or_path=<PATH_TO_FINETUNED_MODEL> \
    model.max_seq_length=${MAX_SEQ_LENGTH} \
    generation.batch_size=${BATCH_SIZE} \
    samba_run.pef=${PEF}
----

== Task 3: Data Parallel Training

Data parallel training can result in significant performance improvements, so we're including those instructions. See https://docs.sambanova.ai/runtime/latest/architecture.html#_data_parallel_applications[Data Parallel Applications] for a high-level overview of how data parallel applications work.

See the individual model cards and review their data parallel capabilities.

* xref:sambanova_modelzoo/models/llama/README.adoc[Llama model card]
* xref:sambanova_modelzoo/models/mistral/README.adoc[Mistral model card]
* xref:sambanova_modelzoo/models/gemma/README.adoc[Gemma model card]

=== Step 1: Compile for Data Parallel Training

The graph must be compiled explicitly to work with data parallel. Under the hood, the compiler annotates the gradient symbols in the PEF and adds one or more reduce operand buffers to the graph that will be used during the gradient synchronization. Some grouping is applied to the gradient symbols produced by a section to increase efficiency of the reduce operations. The group symbols are annotated as gradients rather than the sub-symbols. If you are interested in learning more about how this works, see https://docs.sambanova.ai/developer/latest/data-parallel.html#_what_is_data_parallel[What is Data Parallel].

To compile for data parallel training: 

[source,bash]
----
python rdu_train_llm_dp.py \
    command=compile \
    checkpoint.config_name=${CHECKPOINT} \
    model.max_seq_length=${MAX_SEQ_LENGTH} \
    training.batch_size=${BATCH_SIZE} \
    samba_compile.arch=${ARCH}
----

* Before compilation, check the version of the `sambanova-runtime` package on the host machine by running the following command outside the container (use either rpm or dpkg):
+
[source,bash]
----
(rpm -q sambanova-runtime 2>/dev/null || dpkg -s sambanova-runtime 2>/dev/null) | egrep -m 1 -o "[0-9]+\.[0-9]+\.[0-9]+"
----
* If your host machine is on an older version of sambanova-runtime than the PEF, add `+samba_compile.target_sambaflow_version=MAJOR.MINOR.PATCH` to your command. 

+
NOTE: When using the Hydra command-line syntax, you must add a `+` for arguments not defined in `config/base_config_rdu.yaml`. For parameters already in the YAML file, you can either modify the YAML directly or override them on the command line without the `+`. See link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[How to pass in arguments] for additional guidance.

=== Step 2: Train the Model with Multiple Workers

We use `torch.distributed`, compiled with MPI support, to handle application launching and basic communication between the data parallel (DP) replicas. Any `torch.distributed` calls may be used in a DP app. We have our own collectives communication library (CCL) that currently:

* Implements accelerated all-reduce and all-gather functions using RoCE (RDMA over Converged Ethernet) and/or local PCIe DMA for data transfers. 
* Performs the gradient averaging by executing bitfiles on the RDU.

Launch data parallel apps with the standard MPI launcher, mpirun, or with another MPI-compliant launcher, such as Slurm. We use the MPICH-3.4.3 MPI library, which is installed as part of SambaFlow. 
Remember to set `OMP_NUM_THREADS` if there are resource limits and notice we need to set `checkpoint.model_name_or_path` rather than `checkpoint.config_name` to load the checkpoint.

[source,bash]
----
export DATASET=./ultrachat_dialogue;  # or container path to dataset
export PEF=/path/to/compiled/pef;
/opt/mpich-3.4.3/bin/mpirun -np <NUM_PROCESSES> python -u rdu_train_llm_dp.py \
    command=run \
    checkpoint.model_name_or_path=${CHECKPOINT} \
    model.max_seq_length=${MAX_SEQ_LENGTH} \
    samba_run.pef=${PEF} \
    training.dataset=${DATASET}
----

After several minutes, you'll see logs like the following, which indicate that the model begins the training run:

[source,bash]
----
Number of epochs: 1
Per worker batch size: 16
Per worker number of batches: 2,622
Per worker number of sequences: 41,952
Number of DP workers: 2

DP: 2-way, Epoch [1/1], Step [1/2622], Loss: 11.1906
----

By default, evaluation runs at the end of each training epoch in `rdu_train_llm_dp.py`. To change this behavior, you may set `evaluate` to `False` in the `config/base_config_rdu_dp.yaml`. During evaluation, both loss values and perplexity are calculated on the validation dataset. Perplexity helps you evaluate how well a probability distribution predicts a sample. In the context of generative AI, it quantifies how "surprised" the model is by a given input, based on the data it has been trained on. A lower perplexity indicates that the model is less surprised and thus better at predicting the input. See link:https://guides.library.unlv.edu/c.php?g=1361336&p=10054021[this document] for background. 

NOTE: Perplexity information is available only when you run training in data parallel mode. 

== Task 4: Make changes to the Model

A key experience of Model Zoo is making changes to the model source code and parameters to best fit your usecase. The link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html#_making_changes_to_model_zoo_models[Making changes to Model Zoo models] section in our Best Practices doc goes into this in detail. We encourage you to check that out and experiment with different configurations!

== Troubleshooting

// TODO: talk more about this, or point to Troubleshooting in the doc set. 
For additional logging to assist with debugging compilation, add the following flag to the compile command.
[source,bash]
----
+samba_compile.debug=True +samba_compile.verbose=True
----

For additional Troubleshooting information, see link:https://docs.sambanova.ai/developer/latest/modelzoo-troubleshooting.html[
troubleshooting].

== See Also

* See the xref:text_generation/README.adoc[/text_generation README] and the xref:training/README.adoc[/training README] for a Quick Run summary of those commands and for a discussion of differences and communalities between the model on CPU and RDU.  
* See the README files for each model in `sambanova_modelzoo/nlp` for some details about each supported model. 
* See link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[Model Zoo best practices] for a discussion of making changes to a model, a list of tested checkpoints, and more. 
