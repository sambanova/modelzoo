# Copyright 2024 SambaNova Systems, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

defaults:
    # Removed temporarily
    # - nlp_default_args
    - _self_

hydra:
  searchpath: 
    - pkg://sambanova_modelzoo/libs/nlp/args
    - src/sambanova_modelzoo/libs/nlp/args
# Model configs, changes here are effective only if the PEF is compiled/re-compiled.
model:
    # Toggle to control the normalization layer precision. True means fp32 and false means bf16.
    fp32_ln: False
    # When this flag is True, logits are returned in fp32 precision. 
    # When this flag is False, logits are returned  bf16 precision. 
    fp32_logits: True
    # In LLMs, when there are multiple hidden layers, each layer hidden state is accumulated together. 
    # This flag controls whether the accumulation occurs in fp32 or bf16.
    fp32_skip_add: True
    # When this flag is False, the attention layer is performed in bf16. 
    # When this flag is True, the attention layer uses both fp32 and bf16 precision. 
    # See https://docs.sambanova.ai/developer/latest/mixed-precision.html
    mixedp_attn: True
    # The max number of tokens that the RDU graph can generate minus the input length.
    # The sequence length includes the input prompts and the generated tokens. 
    # At runtime, the length of input prompts must be smaller than max sequence length.
    # Set in the command line for o1hd run for longer seq lengths.
    max_seq_length: 4096
    #
    # Flags needed for o1hd models
    run_early_tp: True
    use_plugin_heuristics: True

checkpoint:
    # Path to the downloaded Hugging Face cache, including checkpoints, model config and tokenizer.
    model_name_or_path: null

# A string of either compile or run.
command: null

# Compiler configs for PEF optimization and generation.
# Changes here will only be reflected if the PEF is compiled/re-compiled.
# NOTE: only need to change arch/output_folder and keep the others as default.
samba_compile:
    # Required! Target RDU architecture to compile to: sn20, sn30, sn40. 
    arch: null
    # Output folder for the PEF and other artifacts. If null, defaults to the current working directory. 
    output_folder: null
    # The name of PEF file. If unspecified, we generate a name that's based on the app, PID, and current time. 
    pef_name: null
    # Optimization level, o1 fuses adjacent operators for better performance. 
    # The default O3 uses the full graph optimization scope and compiles more slowly.
    # See https://docs.sambanova.ai/developer/latest/compiler-o1.html for background information. 
    optim_level: o1
    # Whether to compile inference graph only. It should be always True for text generaton tasks.
    inference: True
    #
    # The SambaFlow compiler performs blfoat16 computation by default. 
    # This flag forces operators to be in float32/bfloat16 mixed precision.
    # See https://docs.sambanova.ai/developer/latest/mixed-precision.html for background information. 
    enable_mixed_precision_ops:
        - gemm
        - softmax
    # Accumulation mode for tiling, bf16sr means bf16 inputs with stochastic rounding.
    tiling_accum: bf16sr
    # A legacy flag for CrossEntropyLoss.
    use_air_ce: True
    # Enable HBM allocation for sn40
    all_regions_hbm: True
    # Number of tiles to be used on chip. If you don't set num_tiles=4 on an SN30 system,
    # the compiler uses tensor parallel mode, which is not currently supported with Model Zoo. 
    num_tiles: 4
    # HBM/DRAM auto allocation algorithm
    hbm_auto_alloc: linear_scan
    # number of chips for TP execution
    n_chips: 8
    # o1hd: enable early_tp
    run_early_tp: True
    # o1hd: compiler configs file
    compiler_configs_file: /opt/software/sambaflow/apps/modelzoo/examples/nlp/text_generation/config/o1hd_config.json
    # o1hd flag
    optimize_concat_split: True
    # o1hd flag
    enable_distribution_pass: True
    # o1hd: specify resource scaling 
    resources_scaling_factors:
        - "4"
        - "4"
        - "4"
    # o1hd: pcu scaling 
    enable_pcu_scaling: True
    # o1hd flag 
    skip_pnr_aware: True
    # o1hd: enable section looping 
    loop_conversion: True
    # o1hd flag
    enable_coe: True

# Generation runtime configs
samba_run:
    # The path to the compiled PEF file.
    pef: null

# Application runtime configs
generation:
    # The batch size to compile a static graph. If you change this parameter you have to recompile. 
    # This parameter can also be set in command-line for other batch sizes.
    batch_size: 1
    # Preset prompt text. The compiler expects that the number of prompts matches the batch_size that is specified
    # in the model configuration.
    # By default a single prompt of "Once upon a time" is provided for
    # batch_size of 1. For larger batch_size, user must add additional prompts.
    prompts:
        - 'Once upon a time'
    #   - 'This is a good story'
    #   - 'I would like to discuss'
    #   - 'Yesterday, when I as walking'
    # Maximum tokens to be generated. By default (max_seq_length - prompt_length) tokens.
    max_new_tokens: 32
    # Random seed for torch, numpy and python library
    seed: 12345
    output_dir: 'text_gen_telemetry'
