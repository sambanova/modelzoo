= Gemma model card

Gemma, is "a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models." This folder contains source code that adapts the model for running on RDU. 

== Architecture changes 

Model Zoo models maintain checkpoint compatibility with Hugging Face, so there are no architecture differences. 

* See  xref:patch_gemma.py[] for details on how we've modified the model to run on RDU
* See xref:../../examples/nlp/training/README.adoc[the training README] and the  xref:../../examples/nlp/text_generation/README.adoc[text generation README] for discussions of CPU/RDU differences. 

== Gemma model variations

[cols="2,2,4,2,3,3,1,4", options="header"]
|===
| Model | Parameter Count | HW Generation | Training/Inference | Sequence Length | Batch Size | TP Configurations | O1HD (TP8 and SN40 only)
| Gemma | 7B | SN30, SN40 | Inference | 4096, 8192 | 1, 4, 8, 16 | 1 | No
| Gemma | 7B | SN30, SN40 | Training | 4096, 8192 | 1, 2, 4, 8 | 1 | No
|===

NOTE: Tensor Parallelism (TP) is a distributed computing technique that splits a model across multiple devices, enabling them to process different parts of the computation in parallel. See link:https://docs.sambanova.ai/developer/latest/tensor-parallel.html[tensor parallel] for more information. It is also known as 'n_chips', e.g. TP8 is 'n_chips 8' and TP16 is 'n_chips 16'.

NOTE: O1HD, also known as plugin heuristics, involves applying specialized optimizations to customize the model's mapping on the RDU, improving its performance and execution efficiency.

== General use cases

See the link:https://huggingface.co/google/gemma-7b-it[Gemma Model Card] and the link:https://ai.google.dev/gemma/docs[Gemma models overview]. SambaNova does not ship checkpoints for this model. Instead, you can use the Model Zoo model source with the corresponding Hugging Face checkpoint. 

== Data preparation

Our models expect data in HDF5 format. See the SambaNova data preparation scripts in link:https://github.com/sambanova/generative_data_prep[this public GitHub repo]. 

== Model parameters and configuration

See link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[Model Zoo Best Practices] for a discussion of modification options and limitations.  

== Supported models
Model Zoo currently supports the following Gemma versions: 

* Gemma-7b

link:https://docs.sambanova.ai/developer/latest/modelzoo-best-practices.html[Model Zoo Best Practices] lists the checkpoints that we have tested, but you can use other Hugging Face checkpoints for the corresponding model. 

== Data parallel functionality

Data-parallelism is a method where the dataset is split into several parts, and each part is processed at the same time by different replicas of the application.

The following Gemma models support 8-way data parallel functionality:

* Gemma-7b

See xref:developer::data-parallel.adoc#_what_is_data_parallel[What is data parallel] for more information.
